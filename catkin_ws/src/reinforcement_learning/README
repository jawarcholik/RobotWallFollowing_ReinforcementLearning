Jacob Warcholik

This package is a naive implementation of a reinforcement learning simulation to teach the triton robot to follow a wall.
The implementation is naive as there is no reward function and the Q-Table values do not get updated they are initially
set using some basic assumptions on the likelihood an action will be beneficial given the state that is observed.
In the source folder is the q_learning.py script. This script defines an environment and a robot agent. In addition there
is a definition for the state space and action space along with the naive definition of the Q-Table. The robot agent subscribes
to the /scan topic to get information on the distances from the walls the robot is. It has a publisher node which sends Pose2D
messages to the /triton_lidar/vel_cmd topic. This allows the script to send messages to the gazebo simulation to move the robot.

Compile and Run:

To compile this package put the reinforcement_learning folder in the src folder of a working catkin workspace and run
the catkin_make command. The launch file in this package calls a launch file located in the stingray_sim package as well.
With both of the necessary packages in the workspace the simulation can be run by running:

roslaunch reinforcement_learning wall_follow_reinforcement_simple.launch

Once Gazebo opens the user is prompted with an option to train by inputting 0 or to test by inputting 1. If the user chooses
to train then the triton_lidar robot will begin a TD reinforcement learning algorithm for 150 episodes. The terminal will
show the start of every 5th episode. Once the training is complete the user will be prompted again if they would like to train
or test. The test option will spawn the robot at a random location at which point it will attempt to follow the first right hand wall it
finds.

Improvements:

To make it more user friendly the user is now prompted at the end of the training period for the file path they would like to save the
q-table too. In addition many changes were made to the reward function to make it simpler and more effective. The prior Knowledge
in the predefined q-table was also adjusted. Finally, major changes were made to the orientation state variable so that it was using
linear regression instead of comparing to past states. These improvements allowed the robot to learn three of the turning scenarios.

It was still found to be difficult to get the robot to learn to turn right 90 degrees at the L corner and to turn right 180
degrees at an I corner.
